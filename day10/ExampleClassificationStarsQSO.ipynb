{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExampleClassificationStarsQSO.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqkOstLOLLHu",
        "colab_type": "text"
      },
      "source": [
        "# This tutorial walks through an example classification with mock color-color data of stars and QSO's (Quasi-stellar object or quasar).  We use logistic regression as the \"Machine Learning\" tool for classification.\n",
        "\n",
        "## Author: Camille Avestruz \n",
        "\n",
        "## Date Created: July 2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Wdtej_Hzts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  This is where we import \"packages\". Think of these as pre-written bits of code (that are known to already work!) \n",
        "#      that we can just use.  \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N-sXSPULXSW",
        "colab_type": "text"
      },
      "source": [
        "# Learning goal 1 - What does a classifier do?\n",
        "## Let's start with some example data.  \n",
        "\n",
        "Below is a plot of stars and quasars in color space: g-r color vs. u-g"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPzGrJL-LV2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate fake g-r and u-g data that kind of looks like the SDSS plot \n",
        "#   (avoid requiring access astroML this morning)\n",
        "\n",
        "gr_qso_vals = np.random.normal(0.1, 0.2, 300)\n",
        "ug_qso_vals = np.random.normal(0.2, 0.3, 300)\n",
        "\n",
        "\n",
        "gr_stars_vals = np.random.normal(0.8, 0.2, 1000)\n",
        "ug_stars_vals = np.random.normal(.8, 0.25, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55sLFVNQLkr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(ug_qso_vals, gr_qso_vals, '.', ms=2, c='r', label='QSO')\n",
        "\n",
        "plt.plot(ug_stars_vals, gr_stars_vals, '.', ms=2, c='b', label='stars')\n",
        "\n",
        "plt.xlabel('u-g color',fontsize='xx-large')\n",
        "plt.ylabel('g-r color',fontsize='xx-large')\n",
        "plt.xlim(-.5,2.5)\n",
        "plt.ylim(-1,1.5)\n",
        "plt.legend(fontsize='large')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfscZwupy0XQ",
        "colab_type": "text"
      },
      "source": [
        "## Note, the above plot is color coded, based on whether a data point corresponds to a QSO or star.  i.e. The data is labeled.  \n",
        "\n",
        "## 1 min discussion with your neighbor: How would you pick out stars and QSOs from the follow unlabeled dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuNsDPHrLl5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gr_vals_no_label = np.concatenate((np.random.normal(0.1, 0.2, 100), np.random.normal(0.8, 0.2, 100)))\n",
        "ug_vals_no_label = np.concatenate((np.random.normal(0.2, 0.3, 100), np.random.normal(.8, 0.25, 100)))\n",
        "\n",
        "plt.plot(ug_vals_no_label, gr_vals_no_label, '.', ms=2, c='k', label='no label')\n",
        "\n",
        "plt.xlabel('u-g color',fontsize='xx-large')\n",
        "plt.ylabel('g-r color',fontsize='xx-large')\n",
        "plt.xlim(-.5,2.5)\n",
        "plt.ylim(-1,1.5)\n",
        "plt.legend(fontsize='large')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFf-mDdYzA24",
        "colab_type": "text"
      },
      "source": [
        "## What were your assumptions? (1min)\n",
        "\n",
        "### Let's now build a model based on some assumptions (to discuss further later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AawwKx6wQSTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the model we will use to draw a decision boundary\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# We create an instance of the model (still untrained) here.\n",
        "model = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3sAyD9KzFcA",
        "colab_type": "code",
        "outputId": "ef62f4d5-bb6b-4b0b-e278-4cfe9b2b4605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# We have to train our model with our labeled data.  Feature values are commonly named X_?, \n",
        "#    and label values are commonly named y_?\n",
        "\n",
        "# Feature 1\n",
        "X_data_gr = np.concatenate((gr_qso_vals, gr_stars_vals)) \n",
        "print('g-r feature has ',X_data_gr.shape, 'values (column)')\n",
        "\n",
        "# Feature 2\n",
        "X_data_ug = np.concatenate((ug_qso_vals, ug_stars_vals))\n",
        "print('u-g feature has ',X_data_ug.shape, 'values (column)')\n",
        "\n",
        "# Features\n",
        "X_data = np.array([X_data_gr, X_data_ug]).T\n",
        "print('The features are of shape, ',X_data.shape, ' (two columns 1300 rows)')\n",
        "\n",
        "# Labels - we will use 0 to indicate QSO, and 1 to indicate star.  (e.g. Is this a star?  False vs. True.)\n",
        "y_data = np.concatenate( (np.zeros(len(gr_qso_vals)), 1+np.zeros(len(gr_stars_vals))) )\n",
        "print('The labels are of length ',len(y_data), ', which is the total number of labeled data points we have.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "g-r feature has  (1300,) values (column)\n",
            "u-g feature has  (1300,) values (column)\n",
            "The features are of shape,  (1300, 2)  (two columns 1300 rows)\n",
            "The labels are of length  1300 , which is the total number of labeled data points we have.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe_ixzy0zT7_",
        "colab_type": "code",
        "outputId": "02989a2a-af3e-4abc-a35e-69808ce618b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#  We train the model with our labeled data, but first we have to split the data (randomly) into some that will \n",
        "#    be used to train the model, and some that will be saved to later assess how well our model does on data it \n",
        "#    has never seen before\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.30)\n",
        "\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xy18pG_0VU-",
        "colab_type": "text"
      },
      "source": [
        "# Let's visualize the decision boundary we just made.\n",
        "Visualize the decision boundary:  https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html?fbclid=IwAR190kLQgXQIIjC6n5ZyPIZAu_PigFJAnyEP2PkXdgnGiUe8fKbmZzQHmgE#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k3JC-8czWfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_boundary_of_trained_model_and_data(X_to_plot, y_to_plot, legend_data_name, trained_model,\n",
        "    h = .02, xaxis_min = -.5, xaxis_max = 2.5, yaxis_min = -1., yaxis_max = 1.5): \n",
        "    '''This function is a visualization code to see the decision boundary compared with our data \n",
        "    based on code from the link above.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_to_plot : array-like\n",
        "        features, should be at least 2-d for 2-d visualization\n",
        "    y_to_plot : array-like\n",
        "        one dimensional labels either 0 or 1\n",
        "    legend_data_name : str\n",
        "        name of data you are overplotting \n",
        "    trained_model : sklearn.linear_model\n",
        "        trained model\n",
        "    h : float \n",
        "        step size of the mesh grid used to visualize areas of decision\n",
        "    xaxis_min : float\n",
        "        optional \n",
        "    xaxis_max : float\n",
        "        optional\n",
        "    yaxis_min : float\n",
        "        optional \n",
        "    yaxis_max : float\n",
        "        optional\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib figure\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    xx, yy = np.meshgrid(np.arange(xaxis_min, xaxis_max, h), np.arange(yaxis_min, yaxis_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X_to_plot[:, 0], X_to_plot[:, 1], c=y_to_plot, edgecolors='k', cmap=plt.cm.Paired, label=\"Training data\")\n",
        "    plt.xlabel('g-r', fontsize='xx-large')\n",
        "    plt.ylabel('u-g', fontsize='xx-large')\n",
        "\n",
        "    plt.xlim(xaxis_min, xaxis_max)\n",
        "    plt.ylim(yaxis_min, yaxis_max)\n",
        "\n",
        "    plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzYIIblH0VvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Here we visualize the decision boundary of the trained model, and overplot the training data used to train \n",
        "#    that model\n",
        "visualize_boundary_of_trained_model_and_data(X_train, y_train, 'Training data', model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUGh9JBX0gCZ",
        "colab_type": "text"
      },
      "source": [
        "## How well do you think we did above?\n",
        "###  There are multiple ways to *quantify* how well we did with our test set.  Let's first visually see how well the test set did, then use a *metric* to quantify how well the test set did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHf6NcuI0dp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Here we show where the test data lies (which was NOT used to train the model)\n",
        "visualize_boundary_of_trained_model_and_data(X_test, y_test, 'Training data', model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2pbg-QH0kQ2",
        "colab_type": "text"
      },
      "source": [
        "## Let's now quantify how well our model is doing with a metric.  Recall the ROC curve from the slides (1 is a perfect classifier, and 0.5 is a classifier that is as good as a coin flip!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnYR7t3i0gWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print('When evaluating our model on the training set, our model has an AUC of:  ',  \n",
        "      roc_auc_score(y_train, model.predict(X_train)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB2rcQ850vEN",
        "colab_type": "text"
      },
      "source": [
        "#  Do you think the AUC of the model evaluated on the test set should be higher, lower, or the same as the AUC of the model evaluated on the training set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcqTd_Jg0sDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('When evaluating our model on the training set, our model has an AUC of:  ',  \n",
        "      roc_auc_score(y_test, model.predict(X_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz9LLgc80zzj",
        "colab_type": "text"
      },
      "source": [
        "#  2 min discussion:  \n",
        "## What happens if stars and qso's overlapped even more, how would our model \"performance\" change?  \n",
        "## How might we improve the model performance?  (Hint:  Think about the movie likes/dislikes from the slides?  What if we only knew about Zootopia and Moonlight, but not Deadpool preferences?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZh5bOb904a8",
        "colab_type": "text"
      },
      "source": [
        "#  Bonus Exercise:  Look at how we constructed the train/test split and evaluated the model on new data.  What if the parameters of the normal distribution were different between the labeled data used to train the model and the new data?  See how the model performs.  (Possible real astro reasons for this:  Sample Selection Bias.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML9PbCSF0vc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIOpybUn00H2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}